{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vCaA1G1C_-gZ"
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "plt.style.use(style=\"seaborn\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## source files combiner script\n",
    "\n",
    "from os import path, getcwd, listdir\n",
    "from os.path import join\n",
    "\n",
    "## all source \"folders\" should be in resource folder ( e.g ./resource/kickstarter...)\n",
    "current_path = join( getcwd(), \"resource\")\n",
    "## all files will be combined to all_data.csv\n",
    "combined_data_set_path = join( getcwd(), \"kickstarter_data_set\", \"all_data.csv\" )\n",
    "\n",
    "combined_file = open(combined_data_set_path, \"w\", encoding=\"UTF-8\")\n",
    "\n",
    "dirs = listdir(current_path)\n",
    "\n",
    "for direc in dirs:\n",
    "    c_dir_path = join(current_path, direc)\n",
    "    f_count = 0\n",
    "    for f in listdir(c_dir_path):\n",
    "        f_count +=1\n",
    "        print(f\"Filfe -> {f_count}\")\n",
    "        f_ = open(join(c_dir_path, f), \"r\", encoding=\"UTF-8\")\n",
    "        count = 0\n",
    "        for line in f_.readlines():\n",
    "            count+=1\n",
    "            if f_count == 1 and count == 1:\n",
    "                combined_file.write(line)\n",
    "\n",
    "            if count==1:\n",
    "                continue\n",
    "            else:\n",
    "                combined_file.write(line)\n",
    "                combined_file.flush()\n",
    "\n",
    "combined_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "for path, subdirs, files in os.walk('resource'):\n",
    "    for name in files:\n",
    "        csv_file_path = os.path.join(path, name)\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        all_df = all_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = all_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-OVpBIY_-gc",
    "outputId": "2570258f-ef2f-42cc-8a06-5c706634d361",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# csv_file_path = ('data/kickstarter_data_with_features.csv')\n",
    "# dataframe = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining  some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2oNlkK-r_-gn"
   },
   "outputs": [],
   "source": [
    "# Multiple functions for cleaning data \n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "def remove_number(text):\n",
    "    newstring = re.sub(r'[0-9]+', '', text)\n",
    "    return newstring\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", string)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # nltk.download('stopwords') \n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def get_vocab_size(text):\n",
    "    \"\"\"Count unique words\"\"\"\n",
    "    count = Counter()\n",
    "    for i in text.values:\n",
    "        i = remove_punct(i)\n",
    "        i = remove_emoji(i)\n",
    "        i = remove_URL(i)\n",
    "        i = remove_html(i)\n",
    "        i = remove_number(i)\n",
    "        for word in i.split():\n",
    "            count[word] += 1\n",
    "    return len(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xLf57Jab_-gp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe['blurb']=dataframe['blurb'].astype(\"string\") \n",
    "dataframe = dataframe[['blurb','state', 'pledged']] # only using these 2 cols\n",
    "dataframe = dataframe[dataframe['state'].isin(['successful', 'failed'])]\n",
    "dataframe['state'] = dataframe['state'].replace({'failed': 0, 'successful': 1})\n",
    "dataframe = dataframe.dropna() \n",
    "dataframe['blurb'] = dataframe['blurb'].map(remove_stopwords) # remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blurb</th>\n",
       "      <th>state</th>\n",
       "      <th>pledged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aquaponic system able grow large plants, insid...</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>creating artistic edibles sweet twist. please ...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bicycle® deluxe. 56 luxury hand-illustrated pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>27473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pup pops water-based natural pops dogs healthy...</td>\n",
       "      <td>0</td>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>limited time original works anime-style art fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>back moon little star, cute illustrated book y...</td>\n",
       "      <td>1</td>\n",
       "      <td>473.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>contiene una foto per ogni giorno dell’anno, l...</td>\n",
       "      <td>1</td>\n",
       "      <td>10049.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>anti-bullying rhyme picture book ugliest dog l...</td>\n",
       "      <td>1</td>\n",
       "      <td>35828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>i'm raising funds publish print first book poe...</td>\n",
       "      <td>0</td>\n",
       "      <td>409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>materialise sculptures even newly captured pok...</td>\n",
       "      <td>1</td>\n",
       "      <td>82545.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  blurb  state  pledged\n",
       "0     aquaponic system able grow large plants, insid...      0     23.0\n",
       "1     creating artistic edibles sweet twist. please ...      0     30.0\n",
       "2     bicycle® deluxe. 56 luxury hand-illustrated pl...      1  27473.0\n",
       "3     pup pops water-based natural pops dogs healthy...      0    525.0\n",
       "4     limited time original works anime-style art fr...      1   1131.0\n",
       "...                                                 ...    ...      ...\n",
       "3649  back moon little star, cute illustrated book y...      1    473.5\n",
       "3650  contiene una foto per ogni giorno dell’anno, l...      1  10049.0\n",
       "3651  anti-bullying rhyme picture book ugliest dog l...      1  35828.0\n",
       "3652  i'm raising funds publish print first book poe...      0    409.0\n",
       "3653  materialise sculptures even newly captured pok...      1  82545.4\n",
       "\n",
       "[208506 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C7k1a9Fr_-gs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataframe.drop(['state', 'pledged'], axis=1)\n",
    "Y = dataframe[['state', 'pledged']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Vocabular size: # of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fRD4aw7L_-gs"
   },
   "outputs": [],
   "source": [
    "train_v_size = get_vocab_size(X_train.blurb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108941"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_v_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequencing the text\n",
    "\n",
    "Fitting a tokenizer to assign indices to words and converting the text to sequence of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tokenizer on training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train.blurb)\n",
    "#get train sequences\n",
    "train_seqs = tokenizer.texts_to_sequences(X_train.blurb)\n",
    "train_seqs_max_size = max([len(seq) for seq in train_seqs])\n",
    "#get test sequences\n",
    "test_seqs = tokenizer.texts_to_sequences(X_test.blurb)\n",
    "test_seqs_max_size = max([len(seq) for seq in test_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs_max_size, test_seqs_max_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sequences\n",
    "\n",
    "Padding the sequences to have seqencies of equal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F0kfjuwX_-gv"
   },
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_seqs, maxlen=train_seqs_max_size, padding=\"post\", truncating=\"post\")\n",
    "test_padded = pad_sequences(test_seqs, maxlen=train_seqs_max_size, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCJZj60W_-g3",
    "outputId": "709256b2-8fce-405a-ffe5-b22badd0e978",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (145954, 42)\n",
      "Shape of test (62552, 42)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of train {train_padded.shape}\")\n",
    "print(f\"Shape of test {test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = X_train.blurb\n",
    "X_train_tokenized = [[word for word in document.lower().split()] for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "word_model = Word2Vec(X_train_tokenized, vector_size=100)\n",
    "\n",
    "#build matrix \n",
    "embedding_matrix_w2v = np.random.random(((train_v_size) + 1, 100))\n",
    "for word,i in tokenizer.word_index.items():  \n",
    "    try:\n",
    "        embedding_matrix_ft[i] = word_model.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# create layer\n",
    "embedding_layer_w2v = Embedding((train_v_size) + 1, output_dim=100, \n",
    "                            weights=[embedding_matrix_w2v], trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FastText(vector_size=300)\n",
    "ft.build_vocab(X_train_tokenized)\n",
    "ft.train(tokenizer.word_index, total_examples=ft.corpus_count, epochs=10)\n",
    "\n",
    "# build matrix\n",
    "embedding_matrix_ft = np.random.random(((train_v_size) + 1, ft.vector_size))\n",
    "for word,i in tokenizer.word_index.items(): \n",
    "    try:\n",
    "        embedding_matrix_ft[i] = ft.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# create layer\n",
    "embedding_layer_ft = Embedding((train_v_size) + 1, output_dim=300, \n",
    "                            weights=[embedding_matrix_ft], trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_keras = Embedding(train_v_size, output_dim=100, input_length=train_seqs_max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iYENlcw_-g3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(embeddings, classification=True):\n",
    "    model = Sequential()\n",
    "    model.add(embeddings)\n",
    "    model.add(LSTM(20, dropout=.9))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    adam_opt = Adam(learning_rate=3e-4)\n",
    "    if classification:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
    "    else: \n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=adam_opt, metrics=[\"mse\"])\n",
    "        \n",
    "    return model \n",
    "\n",
    "def train_model(model, train_padded, test_padded, y_train, y_test):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    history = model.fit( train_padded, y_train, epochs=20,\n",
    "                            validation_data=(test_padded, y_test), callbacks=[early_stopping])\n",
    "    \n",
    "    return history\n",
    "    \n",
    "    \n",
    "def evaluate_model(model, test_padded, y_test):\n",
    "    results = model.evaluate(test_padded, y_test, batch_size=128)\n",
    "    return results \n",
    "\n",
    "def plot_history(history):\n",
    "    metrics_df = pd.DataFrame(history.history)\n",
    "    metrics_df[[\"loss\",\"val_loss\"]].plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM -  Predict if a project/campaign will be successful or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = lstm_model(embedding_layer_keras)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['state'], y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_lstm_status_history = pd.DataFrame(history.history)\n",
    "keras_lstm_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(embedding_layer_w2v)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['state'], y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_lstm_status_history = pd.DataFrame(history.history)\n",
    "w2v_lstm_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(embedding_layer_ft)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['state'], y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_lstm_status_history = pd.DataFrame(history.history)\n",
    "ft_lstm_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM - Predict the amount of money collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(embedding_layer_keras, classification=False)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['pledged'], y_test['pledged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['pledged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(embeddings, classification=True):\n",
    "    model = Sequential()\n",
    "    model.add(embeddings)\n",
    "    model.add(Conv1D(10, 3, activation='relu', kernel_regularizer=L2(0.1), bias_regularizer=L2(0.9)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam_opt = Adam(learning_rate=3e-4)\n",
    "    if classification:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=adam_opt, metrics=[\"accuracy\"])\n",
    "    else: \n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=adam_opt, metrics=[\"mse\"])\n",
    "        \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN -  Predict if a project/campaign will be successful or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = cnn_model(embedding_layer_keras)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['state'], y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_cnn_status_history = pd.DataFrame(history.history)\n",
    "keras_cnn_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model(embedding_layer_w2v)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['state'], y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cnn_status_history = pd.DataFrame(history.history)\n",
    "w2v_cnn_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model(embedding_layer_ft)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['state'], y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_cnn_status_history = pd.DataFrame(history.history)\n",
    "ft_cnn_status_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN - Predict the amount of money collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model(embedding_layer_keras, classification=False)    \n",
    "history = train_model(model, train_padded, test_padded, y_train['pledged'], y_test['pledged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_padded, y_test['pledged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "https://keras.io/api/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
