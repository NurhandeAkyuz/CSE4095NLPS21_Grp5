{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_deep_learning_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPn_1ES7zLwa",
        "outputId": "17620b78-944b-4037-d8cf-f44825885644"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip uninstall keras\n",
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=c15a53308f99f04a258782bfc9b06492da027b521f3ad2f407d1a4337ee3936b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires keras>=2.0.0, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, keras-applications, gast, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xNNPMUvCPVM",
        "outputId": "4951fee3-a6c5-46e6-9f8b-41d8f1b4f3f2"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "import os\n",
        "print(keras.__version__)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "from math import nan\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.layers import CRF\n",
        "\n"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-qa4ropox\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-qa4ropox\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=db3e10b3fb81d368873365595033ceb8db2d39141b6dde9f2c966f3dcdb2a38a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-od3xl5je/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQAUGoukCPVU"
      },
      "source": [
        "def get_path(*args):\n",
        "    spec_path = os.sep.join(args)\n",
        "    return os.path.join(os.getcwd(), spec_path)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBwIEj3iwcD",
        "outputId": "13c7dacf-24ec-440c-a969-be88a9929860"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGfbmNhxCPVW",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "327f32c2-ed77-49cf-d0c9-82b630c8a847"
      },
      "source": [
        "# read file and create data frame from it\n",
        "will_parsed_row_of_corpus = 50000\n",
        "file_path = get_path(\"drive\",\"MyDrive\",\"TWNERTC_TC_Coarse Grained NER_DomainIndependent_NoiseReduction.DUMP\") \n",
        "\n",
        "f = open(file_path, \"r\")\n",
        "\n",
        "line_count = 0\n",
        "\n",
        "data = dict()\n",
        "data[\"sentence_id\"] = list()\n",
        "data[\"tag\"] = list()\n",
        "data[\"word\"] = list()\n",
        "\n",
        "sentence_id_arr = data[\"sentence_id\"]\n",
        "tag_arr = data[\"tag\"]\n",
        "word_arr = data[\"word\"]\n",
        "\n",
        "\n",
        "for line in f.readlines():\n",
        "\n",
        "    if line_count > will_parsed_row_of_corpus:\n",
        "      break\n",
        "\n",
        "    line_count += 1\n",
        "    # each line seperated by ht (horizontal tabs)\n",
        "    splitted = line.split(\"\\t\")\n",
        "    \n",
        "    if len(splitted) ==3:\n",
        "      tag_split = splitted[1].split(\" \")\n",
        "      word_split = splitted[2].split(\" \")\n",
        "    \n",
        "      for tag, word in zip(tag_split, word_split):\n",
        "          word = word.strip()\n",
        "          if word[len(word)-1] == \"\\n\":\n",
        "              word = word[:-1]\n",
        "          \n",
        "          sentence_id_arr.append(line_count)\n",
        "          tag_arr.append(tag)\n",
        "          word_arr.append(word)\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"sentence_id\", \"tag\", \"word\"])\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>tag</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B-PERSON</td>\n",
              "      <td>Corina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I-PERSON</td>\n",
              "      <td>Casanova</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>O</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>B-LOCATION</td>\n",
              "      <td>İsviçre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>O</td>\n",
              "      <td>Federal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_id         tag      word\n",
              "0            1    B-PERSON    Corina\n",
              "1            1    I-PERSON  Casanova\n",
              "2            1           O         ,\n",
              "3            1  B-LOCATION   İsviçre\n",
              "4            1           O   Federal"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cATxMUWjCPVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9723866a-7e77-4d94-bd96-742515b45805"
      },
      "source": [
        "df.shape[0]"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "844202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R_IO7U_CPVX"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        self.n_sent = 1\n",
        "        self.dataset = dataset\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
        "                                                        s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnW1uotZCPVY"
      },
      "source": [
        "getter = SentenceGetter(df)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55-TAeauCPVY"
      },
      "source": [
        "sentences = getter.sentences"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67euaDS9CPVY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf082c2-2d60-4718-dbcf-e8fc1ee7efbf"
      },
      "source": [
        "print(f\"Number of sentences in data set {len(sentences)}\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in data set 50001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvG8TnmuCPVZ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2d2d80-c256-45ec-908b-0b722623867b"
      },
      "source": [
        "sentences[10]"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Denton', 'B-LOCATION'),\n",
              " (',', 'O'),\n",
              " ('Amerika', 'B-LOCATION'),\n",
              " ('Birleşik', 'I-LOCATION'),\n",
              " (\"Devletleri'nde\", 'I-LOCATION'),\n",
              " ('Teksas', 'B-LOCATION'),\n",
              " ('eyaletinin', 'O'),\n",
              " ('Denton', 'B-LOCATION'),\n",
              " ('bölgesindeki', 'O'),\n",
              " ('bir', 'O'),\n",
              " ('şehirdir', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxzcLIL0CPVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbe6df4-3818-4c6e-f1b3-1400239f65fa"
      },
      "source": [
        "# check length of longest sentence \n",
        "max_len = max([len(s) for s in sentences])\n",
        "print ('Maximum sequence length:', max_len)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90_fre6yCPVa"
      },
      "source": [
        "# visualize the length of sentences in data\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnOBOYDQCPVa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a47a872c-f842-48f8-fdce-2141d01e4fac"
      },
      "source": [
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.show()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEhG1bK0CPVa"
      },
      "source": [
        "# create set of unique words and add ENDPAD as last element\n",
        "words = list(set(df[\"word\"].values))\n",
        "words.append(\"ENDPAD\")"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O87ZeJxyCPVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40f233c-9e63-4497-b099-4e65f1b7f7d9"
      },
      "source": [
        "n_words = len(words) \n",
        "n_words"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY24srhFCPVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1aa6ef-2c00-43fd-8c26-36d38027146c"
      },
      "source": [
        "tags = list(set(df[\"tag\"].values))\n",
        "tags"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I-LOCATION',\n",
              " 'B-ORGANIZATION',\n",
              " 'I-MISC',\n",
              " 'B-PERSON',\n",
              " 'B-LOCATION',\n",
              " 'B-MISC',\n",
              " 'I-ORGANIZATION',\n",
              " 'O',\n",
              " 'I-PERSON']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFrUAWSPCPVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa115d38-2cd5-49ea-f6a2-e2dff74f2cfc"
      },
      "source": [
        "n_tags = len(tags); n_tags"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovb9DJjwCPVc"
      },
      "source": [
        "# creating index dictionaries for words and tags\n",
        "from future.utils import iteritems\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p2bWgKNCPVc"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\",value=n_words - 1)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZGeC-dCPVc"
      },
      "source": [
        "# We append \"O\" tags end of each sentence that smaller than 50.\n",
        "y_idx = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y_idx, padding=\"post\", value=tag2idx[\"O\"])"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48VUazCtCPVd",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d0b5fa-e9ca-4df7-c6ef-1c23d3993fd1"
      },
      "source": [
        "print(y_idx[2])"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 8, 7, 5, 7, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr84bwPLCPVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf2cc21-ad76-4269-9cbb-f01975c3a7b1"
      },
      "source": [
        "len(y) # it should be equal longest sentence length"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlI8-FEYCPVe"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16exyg9XCPVe"
      },
      "source": [
        "# split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSu4lCPiCPVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c4b4ef-0383-45f5-ba20-25ae4e7db6ed"
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "import keras as k\n",
        "import time\n",
        "print(k.__version__)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nyRHpnZkZNf",
        "outputId": "6bf5e11c-1dcf-4061-8a96-40496a963802"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFZDQ0THkW7F"
      },
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def get_word2vec_embeddding_layer(embedding_size, input_shape):\n",
        "    tokenizer = Tokenizer()\n",
        "    # Train Word2Vec model with gensim on the dataset\n",
        "    w2v_model = Word2Vec(words, size=embedding_size, workers=8)\n",
        "\n",
        "    embedding_matrix_w2v = np.random.random(((n_words) + 1, embedding_size))\n",
        "    for word,i in tokenizer.word_index.items():  \n",
        "      try:\n",
        "          embedding_matrix_w2v[i] = w2v_model.wv[word]\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "    # create layer\n",
        "    embedding_layer_w2v = Embedding(n_words+1, output_dim=embedding_size, weights=[embedding_matrix_w2v], input_length=input_shape, trainable=True)\n",
        "    \n",
        "    return embedding_layer_w2v"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swn_R9-bCPVf"
      },
      "source": [
        "# input shape should be equal to the longest input length\n",
        "def prepare_model(input_shape, embedding_size, learning_rate, embedding_spec=0):\n",
        "  input = Input(shape=(input_shape,))\n",
        "  \n",
        "  if embedding_spec == 0: # for keras embeddding\n",
        "      embedding = Embedding(input_dim=n_words, output_dim=embedding_size, input_length=input_shape)\n",
        "  elif embedding_spec == 1: # for word2vec embedding\n",
        "      embedding = get_word2vec_embeddding_layer(embedding_size, input_shape)\n",
        "      \n",
        "  model = embedding(input)\n",
        "  model = Bidirectional(LSTM(units=embedding_size, \n",
        "                          return_sequences=True, \n",
        "                          dropout=0.5, \n",
        "                          recurrent_dropout=0.5, \n",
        "                          kernel_initializer=k.initializers.he_normal()))(model)\n",
        "  model = LSTM(units=embedding_size * 2, \n",
        "              return_sequences=True, \n",
        "              dropout=0.5, \n",
        "              recurrent_dropout=0.5, \n",
        "              kernel_initializer=k.initializers.he_normal())(model)\n",
        "  model = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  # previously softmax output layer\n",
        "\n",
        "  crf = CRF(n_tags)  # CRF layer\n",
        "  out = crf(model)  # output\n",
        "\n",
        "  model = Model(input, out)\n",
        "\n",
        "  adam = k.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "  #model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REurKNOVWjTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9390697-c9da-463f-c211-03c4d74df424"
      },
      "source": [
        "model = prepare_model(max_len,300, 0.01,1)\n",
        "model.summary()"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_23 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_15 (Embedding)     (None, 50, 300)           36048900  \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 50, 600)           1442400   \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 50, 600)           2882400   \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 50, 9)             5409      \n",
            "_________________________________________________________________\n",
            "crf_8 (CRF)                  (None, 50, 9)             189       \n",
            "=================================================================\n",
            "Total params: 40,379,298\n",
            "Trainable params: 40,379,298\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE2dsRYjCPVg"
      },
      "source": [
        "# Saving the best only\n",
        "def train_model(model):\n",
        "  \"\"\"filepath=get_path(\"ner-bi-lstm-td-model-{val_acc:.2f}.hdf5\")\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "  callbacks_list = [checkpoint] -> , callbacks=callbacks_list\"\"\"\n",
        "  history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=20, validation_split=0.2, verbose=1)\n",
        "  return history, model\n",
        "\n",
        "def save_model(model, *attr):\n",
        "  timestamp = int(round(time.time() * 1000))\n",
        "  file_name = \"ner_bi_lstm_crf_model_\"+\"_\".join(attr)+f\"_{timestamp}.h5\"\n",
        "  model_save_path = get_path(file_name)\n",
        "  model.save_weights(model_save_path)"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3cDN3oCPVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "48c676a0-653b-4adf-9f52-4f1194c7749a"
      },
      "source": [
        "history, trained_model = train_model(model)\n",
        "save_model(trained_model,\"50\",\"300\",\"0.01\")"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/20\n",
            " 1280/32000 [>.............................] - ETA: 26:11 - loss: 1.0732 - crf_viterbi_accuracy: 0.6266 - acc: 0.0016"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-247-3d096f22b962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"50\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"300\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"0.01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-246-07c055ab67e0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   callbacks_list = [checkpoint] -> , callbacks=callbacks_list\"\"\"\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxOWxSZ4W2ET"
      },
      "source": [
        "# model evaluation metrics\n",
        "def set_default_evulation_metrics():\n",
        "  TP = {}\n",
        "  TN = {}\n",
        "  FP = {}\n",
        "  FN = {}\n",
        "  \n",
        "  for tag in tag2idx.keys():\n",
        "      TP[tag] = 0\n",
        "      TN[tag] = 0    \n",
        "      FP[tag] = 0    \n",
        "      FN[tag] = 0    \n",
        "\n",
        "\n",
        "def accumulate_score_by_tag(gt, pred):\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  For each tag keep stats\n",
        "  \"\"\"\n",
        "  if gt == pred:\n",
        "      TP[gt] += 1\n",
        "  elif gt != 'O' and pred == 'O':\n",
        "      FN[gt] +=1\n",
        "  elif gt == 'O' and pred != 'O':\n",
        "      FP[gt] += 1\n",
        "  else:\n",
        "      TN[gt] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5Bv6mrCPVh"
      },
      "source": [
        "# Single prediction and verbose results\n",
        "i = 10\n",
        "p = model.predict(np.array([X_test[i]]))\n",
        "p = np.argmax(p, axis=-1)\n",
        "gt = np.argmax(y_test[i], axis=-1)\n",
        "print(gt)\n",
        "print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
        "for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
        "    #\n",
        "    print(\"{:14}: ({:5}): {}\".format(words[w],idx2tag[gt[idx]],tags[pred]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtHVSpNNXIfh"
      },
      "source": [
        "def test_model(model,x_test,y_test):\n",
        "  p = model.predict(np.array(x_test))\n",
        "\n",
        "  from sklearn.metrics import classification_report\n",
        "\n",
        "  print(classification_report(np.argmax(y_test, 2).ravel(), np.argmax(p, axis=2).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))\n",
        "  \n",
        "  set_default_evulation_metrics()\n",
        "  # acumulate the scores by tag\n",
        "  for i, sentence in enumerate(x_test):\n",
        "      y_hat = np.argmax(p[i], axis=-1)\n",
        "      gt = np.argmax(y_test[i], axis=-1)\n",
        "      for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
        "          accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])\n",
        "\n",
        "  for tag in tag2idx.keys():\n",
        "      print(f'tag:{tag}')    \n",
        "      print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
        "      print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pob-9rDRXLdg"
      },
      "source": [
        "test_model(model,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_DphIjLSe-2"
      },
      "source": [
        "# load model from save and test\n",
        "from keras.models import load_model\n",
        "model = prepare_model(50,300,0.01)\n",
        "trained_model_path = get_path(\"ner_bi_lstm_crf_model_50_300_0.01_1621337204296.h5\")\n",
        "model.load_weights(trained_model_path)\n",
        "test_model(model,X_test,y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}