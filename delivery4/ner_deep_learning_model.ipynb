{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_deep_learning_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPn_1ES7zLwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d15132e-ec6e-492d-904d-2cab748d1c9e"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip uninstall keras\n",
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.4\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Uninstalling Keras-2.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/Keras-2.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.2.4\n",
            "Collecting tensorflow==1.15\n",
            "  Using cached https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires keras>=2.0.0, which is not installed.\u001b[0m\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow",
                  "tensorflow_core"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNNPMUvCPVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0c98e1-187d-44ab-97a2-06ed0fd47548"
      },
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import os\n",
        "from math import nan\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.layers import CRF\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "import keras as k\n",
        "print(k.__version__)\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from future.utils import iteritems\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import  crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras.models import load_model\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-lapo4yev\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-lapo4yev\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=d7a59227e3d24683732cd657c1d14733662578a5fad2b7b590954372dcb3e33d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7qeex7qg/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "2.2.4\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGfbmNhxCPVW",
        "scrolled": true
      },
      "source": [
        "max_len_seq = ...\n",
        "vocabulary = ...\n",
        "number_of_words = ...\n",
        "number_of_tags = ...\n",
        "tags = ...\n",
        "word_to_ids = ...\n",
        "tag_to_ids = ...\n",
        "ids_to_tags = ...\n",
        "\n",
        "TP = {}\n",
        "TN = {}\n",
        "FP = {}\n",
        "FN = {}\n",
        "\n",
        "def get_path(*args):\n",
        "    spec_path = os.sep.join(args)\n",
        "    return os.path.join(os.getcwd(), spec_path)\n",
        "\n",
        "# read file and create data frame from it\n",
        "def parse_data(will_parsed_row_of_corpus):\n",
        "\n",
        "  file_path = get_path(\"drive\",\"MyDrive\",\"TWNERTC_TC_Coarse Grained NER_DomainIndependent_NoiseReduction.DUMP\") \n",
        "\n",
        "  f = open(file_path, \"r\")\n",
        "\n",
        "  line_count = 0\n",
        "\n",
        "  data = dict()\n",
        "  data[\"sentence_id\"] = list()\n",
        "  data[\"tag\"] = list()\n",
        "  data[\"word\"] = list()\n",
        "\n",
        "  sentence_id_arr = data[\"sentence_id\"]\n",
        "  tag_arr = data[\"tag\"]\n",
        "  word_arr = data[\"word\"]\n",
        "\n",
        "\n",
        "  for line in f.readlines():\n",
        "\n",
        "    if line_count > will_parsed_row_of_corpus:\n",
        "      break\n",
        "\n",
        "    line_count += 1\n",
        "    # each line seperated by ht (horizontal tabs)\n",
        "    splitted = line.split(\"\\t\")\n",
        "    \n",
        "    if len(splitted) ==3:\n",
        "      tag_split = splitted[1].split(\" \")\n",
        "      word_split = splitted[2].split(\" \")\n",
        "    \n",
        "      for tag, word in zip(tag_split, word_split):\n",
        "          word = word.lower()\n",
        "          word = word.strip()\n",
        "          if word[len(word)-1] == \"\\n\":\n",
        "              word = word[:-1]\n",
        "          \n",
        "          sentence_id_arr.append(line_count)\n",
        "          tag_arr.append(tag)\n",
        "          word_arr.append(word)\n",
        "\n",
        "  df = pd.DataFrame(data, columns=[\"sentence_id\", \"tag\", \"word\"])\n",
        "\n",
        "  print(df.head())\n",
        "  print(f\"data shape -> {df.shape[0]}\")\n",
        "\n",
        "  return df\n",
        "\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        self.n_sent = 1\n",
        "        self.dataset = dataset\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
        "                                                        s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "def analyze_and_preprocess(df, test_size=0.2):\n",
        "  global max_len_seq, vocabulary, number_of_words, number_of_tags, tags, word_to_ids, tag_to_ids, ids_to_tags\n",
        "  \n",
        "  # creating sentence sequences with tags from data (preprocess)\n",
        "  sentences = SentenceGetter(df).sentences\n",
        "  print(f\"Number of sentences in data set {len(sentences)}\")\n",
        "  print(sentences[10])\n",
        "\n",
        "  # it should be static because when we split for just testing max length might be different\n",
        "  # and it will produce error while padding and test will not be finish correctly \n",
        "  # I set it 100 because in TBMM corpus there might be longer sentences than this corpus includes.\n",
        "  max_len_seq = 100\n",
        "\n",
        "  # plot lengths of sequences (sentences) graph\n",
        "  plt.hist([len(s) for s in sentences], bins=max_len_seq)\n",
        "  plt.show()\n",
        "  # create set of unique words and add ENDPAD as last element\n",
        "  vocabulary = list(set(df[\"word\"].values))\n",
        "  vocabulary.append(\"ENDPAD\")\n",
        "\n",
        "  number_of_words = len(vocabulary) \n",
        "  print(f\"Number of unique words (Vocabulay size) -> {number_of_words}\")\n",
        "\n",
        "  # create set of unique tags\n",
        "  tags = list(set(df[\"tag\"].values))\n",
        "  print(\"Tags ->\", tags)\n",
        "\n",
        "  number_of_tags = len(tags)\n",
        "  print(f\"Number of tags -> {number_of_tags}\") \n",
        "\n",
        "  # creating index dictionaries for words and tags\n",
        "  word_to_ids = {w: i for i, w in enumerate(vocabulary)}\n",
        "  tag_to_ids = {t: i for i, t in enumerate(tags)}\n",
        "  ids_to_tags = {v: k for k, v in iteritems(tag_to_ids)}\n",
        "\n",
        "  # fill with padding value (ENDPAD) until being equal all sequence size with longest sequence\n",
        "  X = [[word_to_ids[w[0]] for w in s] for s in sentences]\n",
        "  X = pad_sequences(maxlen=max_len_seq, sequences=X, padding=\"post\",value=number_of_words - 1)\n",
        "\n",
        "  # fill with padding value (O) until being equal all sequence size with longest sequence\n",
        "  y_id = [[tag_to_ids[w[1]] for w in s] for s in sentences]\n",
        "  y = pad_sequences(maxlen=max_len_seq, sequences=y_id, padding=\"post\", value=tag_to_ids[\"O\"])\n",
        "  print(f\"Lenght of y[0] -> {len(y[0])}\") # it should be equal longest sentence length\n",
        "\n",
        "  y = [to_categorical(i, num_classes=number_of_tags) for i in y]\n",
        "\n",
        "  # split data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, vocabulary, tags\n",
        "\n",
        "def get_word2vec_embeddding_layer(embedding_size, num_words,input_shape):\n",
        "    global vocabulary\n",
        "    tokenizer = Tokenizer()\n",
        "    # Train Word2Vec model with gensim on the dataset\n",
        "    w2v_model = Word2Vec(vocabulary, size=embedding_size, workers=8)\n",
        "\n",
        "    embedding_matrix_w2v = np.random.random(((num_words)+1, embedding_size))\n",
        "    for word,i in tokenizer.word_index.items():  \n",
        "      try:\n",
        "          embedding_matrix_w2v[i] = w2v_model.wv[word]\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "    # create layer\n",
        "    embedding_layer_w2v = Embedding(num_words+1, output_dim=embedding_size, weights=[embedding_matrix_w2v], input_length=input_shape, trainable=True)\n",
        "\n",
        "    return embedding_layer_w2v\n",
        "\n",
        "# input shape should be equal to the longest input length\n",
        "def prepare_model( embedding_spec=0):\n",
        "  \n",
        "  max_len = \n",
        "  embedding_size = 300\n",
        "  learning_rate = 0.01\n",
        "\n",
        "  input = Input(shape=(200,))\n",
        "\n",
        "  if embedding_spec == 0: # for keras embeddding\n",
        "      embedding = Embedding(input_dim=num_words, output_dim=embedding_size, input_length=input_shape)\n",
        "        \n",
        "      \n",
        "  model = embedding(input)\n",
        "  model = Bidirectional(LSTM(units=embedding_size, \n",
        "                          return_sequences=True, \n",
        "                          dropout=0.5, \n",
        "                          recurrent_dropout=0.5, \n",
        "                          kernel_initializer=k.initializers.he_normal()))(model)\n",
        "  model = LSTM(units=embedding_size * 2, \n",
        "              return_sequences=True, \n",
        "              dropout=0.5, \n",
        "              recurrent_dropout=0.5, \n",
        "              kernel_initializer=k.initializers.he_normal())(model)\n",
        "  model = TimeDistributed(Dense(num_tags, activation=\"relu\"))(model)  # previously softmax output layer\n",
        "\n",
        "  crf = CRF(num_tags)  # CRF layer\n",
        "  out = crf(model)  # output\n",
        "\n",
        "  model = Model(input, out)\n",
        "\n",
        "  adam = k.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "  #model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# model evaluation metrics\n",
        "def set_dafault_evaluation_parameters():\n",
        "  global TP, FN, FP, TN\n",
        "  for tag in tag_to_ids.keys():\n",
        "      TP[tag] = 0\n",
        "      TN[tag] = 0    \n",
        "      FP[tag] = 0    \n",
        "      FN[tag] = 0    \n",
        "\n",
        "\n",
        "def accumulate_score_by_tag(gt, pred):\n",
        "  global TP, FN, FP, TN\n",
        "  \"\"\"\n",
        "  For each tag keep stats\n",
        "  \"\"\"\n",
        "  if gt == pred:\n",
        "      TP[gt] += 1\n",
        "  elif gt != 'O' and pred == 'O':\n",
        "      FN[gt] +=1\n",
        "  elif gt == 'O' and pred != 'O':\n",
        "      FP[gt] += 1\n",
        "  else:\n",
        "      TN[gt] += 1\n",
        "\n",
        "# Single prediction and verbose results\n",
        "def predict_single_sequence(model, sequence, X_test, y_test):\n",
        "  global vocabulary, ids_to_tags, tags\n",
        "  i = 10\n",
        "  p = model.predict(np.array(sequence))\n",
        "  p = np.argmax(p, axis=-1)\n",
        "  gt = np.argmax(y_test[i], axis=-1)\n",
        "  print(gt)\n",
        "  print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
        "  for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
        "      #\n",
        "      print(\"{:14}: ({:5}): {}\".format(vocabulary[w],ids_to_tags[gt[idx]],tags[pred]))\n",
        "\n",
        "def test_model(model,X_test,y_test):\n",
        "  global ids_to_tags, tags, tag_to_ids, TN, FP, FN, TP\n",
        "\n",
        "  p = model.predict(np.array(X_test))\n",
        "  print(classification_report(np.argmax(y_test, 2).ravel(), np.argmax(p, axis=2).ravel(),labels=list(ids_to_tags.keys()), target_names=list(ids_to_tags.values())))\n",
        "  \n",
        "  # acumulate the scores by tag\n",
        "  for i, sentence in enumerate(X_test):\n",
        "      y_hat = np.argmax(p[i], axis=-1)\n",
        "      gt = np.argmax(y_test[i], axis=-1)\n",
        "      for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
        "          accumulate_score_by_tag(ids_to_tags[gt[idx]],tags[pred])\n",
        "\n",
        "  for tag in tag_to_ids.keys():\n",
        "      print(f'tag:{tag}')    \n",
        "      print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
        "      print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  \n",
        "\n",
        "# Saving the best only\n",
        "def train_model(model):\n",
        "  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "  history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=20, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
        "  return history, model\n",
        "\n",
        "def save_model(model, *attr):\n",
        "  file_name = \"ner_model_with_params_\"+\"_\".join(attr)+\".h5\"\n",
        "  file_path = get_path(file_name)\n",
        "  model.save(file_path)\n",
        "\n",
        "def load_model_from_save(model_file_name):\n",
        "  return load_model(model_file_name,custom_objects={'CRF':CRF, 'crf_loss':crf_loss, 'crf_viterbi_accuracy':crf_viterbi_accuracy})\n",
        "\n",
        "def test_wiht_single_sentence(model, sentence, max_len_seq):\n",
        "  \n",
        "  sentence = sentence.lower()\n",
        "  revised_sentence = \"\"\n",
        "  punc_set = set([',',':',';','\"','(',')','[',']',';','!']) # set('!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~')\n",
        "  for character in sentence:\n",
        "    if character in punc_set:\n",
        "      revised_sentence += \" \" + character\n",
        "    else:\n",
        "      revised_sentence += character\n",
        "\n",
        "  words = sentence.split(\" \")\n",
        "  words_arr = [[word] for word in words ]\n",
        "  len_words = len(words)\n",
        "\n",
        "  for i in range(len_words, max_len_seq):\n",
        "    words.append(\"ENDPAD\")\n",
        "\n",
        "  words = np.asmatrix(words).T\n",
        "  print(words.shape)\n",
        "  p = model.predict(words)\n",
        "  p = np.argmax(p, axis=-1)\n",
        "\n",
        "  res = []\n",
        "  for word, prediction in zip(words, p[0]):\n",
        "    res.append(f\"{word} -> {prediction}\")\n",
        "  \n",
        "  res_str = \" \".join(res)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnW1uotZCPVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e495035d-e3b1-4c50-9ab0-be3a61c1829c"
      },
      "source": [
        "# train model and test\n",
        "# parsing data\n",
        "df = parse_data(100)\n",
        "# data preprocess and splitting\n",
        "X_train, X_test, y_train, y_test, vocabulary, tags = analyze_and_preprocess(df)\n",
        "print(X_test.shape)\n",
        "# model compreparing (compiling)\n",
        "embedding_spec = 0 # keras : 0, word2vec : 1\n",
        "model = prepare_model(len(vocabulary), len(tags), embedding_spec)\n",
        "model.summary()\n",
        "# model training\n",
        "train_model(model)\n",
        "# saving model\n",
        "save_model(model, str(100), str(embedding_size), str(learning_rate), str(number_of_words), str(embedding_spec))\n",
        "# model testing\n",
        "set_dafault_evaluation_parameters()\n",
        "# test model\n",
        "test_model(model,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sentence_id         tag      word\n",
            "0            1    B-PERSON    corina\n",
            "1            1    I-PERSON  casanova\n",
            "2            1           O         ,\n",
            "3            1  B-LOCATION  i̇sviçre\n",
            "4            1           O   federal\n",
            "data shape -> 1744\n",
            "Number of sentences in data set 101\n",
            "[('denton', 'B-LOCATION'), (',', 'O'), ('amerika', 'B-LOCATION'), ('birleşik', 'I-LOCATION'), (\"devletleri'nde\", 'I-LOCATION'), ('teksas', 'B-LOCATION'), ('eyaletinin', 'O'), ('denton', 'B-LOCATION'), ('bölgesindeki', 'O'), ('bir', 'O'), ('şehirdir', 'O'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Number of unique words (Vocabulay size) -> 975\n",
            "Tags -> ['B-MISC', 'B-LOCATION', 'B-PERSON', 'B-ORGANIZATION', 'I-PERSON', 'O', 'I-LOCATION', 'I-ORGANIZATION', 'I-MISC']\n",
            "Number of tags -> 9\n",
            "Lenght of y[0] -> 100\n",
            "(21, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, 100, 300)          292500    \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 100, 600)          1442400   \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 100, 600)          2882400   \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 100, 9)            5409      \n",
            "_________________________________________________________________\n",
            "crf_8 (CRF)                  (None, 100, 9)            189       \n",
            "=================================================================\n",
            "Total params: 4,622,898\n",
            "Trainable params: 4,622,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 64 samples, validate on 16 samples\n",
            "Epoch 1/20\n",
            "64/64 [==============================] - 26s 404ms/step - loss: 2.2262 - crf_viterbi_accuracy: 0.0041 - acc: 0.0098 - val_loss: 0.4165 - val_crf_viterbi_accuracy: 0.9112 - val_acc: 0.9112\n",
            "Epoch 2/20\n",
            "64/64 [==============================] - 8s 125ms/step - loss: 0.5087 - crf_viterbi_accuracy: 0.8947 - acc: 0.0098 - val_loss: 0.4169 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 3/20\n",
            "64/64 [==============================] - 8s 122ms/step - loss: 0.4509 - crf_viterbi_accuracy: 0.9681 - acc: 0.0098 - val_loss: 0.3778 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 4/20\n",
            "64/64 [==============================] - 8s 122ms/step - loss: 0.4147 - crf_viterbi_accuracy: 0.9681 - acc: 0.0098 - val_loss: 0.3176 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 5/20\n",
            "64/64 [==============================] - 8s 126ms/step - loss: 0.3471 - crf_viterbi_accuracy: 0.9681 - acc: 0.0098 - val_loss: 0.2580 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 6/20\n",
            "64/64 [==============================] - 8s 122ms/step - loss: 0.2768 - crf_viterbi_accuracy: 0.9681 - acc: 0.0098 - val_loss: 0.2172 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 7/20\n",
            "64/64 [==============================] - 8s 120ms/step - loss: 0.2248 - crf_viterbi_accuracy: 0.9681 - acc: 0.0098 - val_loss: 0.2180 - val_crf_viterbi_accuracy: 0.9694 - val_acc: 0.9694\n",
            "Epoch 8/20\n",
            "64/64 [==============================] - 8s 121ms/step - loss: 0.2180 - crf_viterbi_accuracy: 0.9672 - acc: 0.0098 - val_loss: 0.1847 - val_crf_viterbi_accuracy: 0.9700 - val_acc: 0.9700\n",
            "Epoch 9/20\n",
            "64/64 [==============================] - 8s 120ms/step - loss: 0.1841 - crf_viterbi_accuracy: 0.9688 - acc: 0.0098 - val_loss: 0.1776 - val_crf_viterbi_accuracy: 0.9700 - val_acc: 0.9700\n",
            "Epoch 10/20\n",
            "64/64 [==============================] - 8s 122ms/step - loss: 0.1770 - crf_viterbi_accuracy: 0.9688 - acc: 0.0098 - val_loss: 0.1798 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 11/20\n",
            "64/64 [==============================] - 8s 118ms/step - loss: 0.1796 - crf_viterbi_accuracy: 0.9688 - acc: 0.0098 - val_loss: 0.1700 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 12/20\n",
            "64/64 [==============================] - 8s 119ms/step - loss: 0.1685 - crf_viterbi_accuracy: 0.9686 - acc: 0.0098 - val_loss: 0.1684 - val_crf_viterbi_accuracy: 0.9719 - val_acc: 0.9719\n",
            "Epoch 13/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-1b68fe573f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# saving model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-2b2df2760751>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjHRly8GmjdH"
      },
      "source": [
        "# reload model and test\n",
        "# ! set number of rows correctly same with trained model\n",
        "# parsing data\n",
        "df = parse_data(100)\n",
        "# data preprocess and splitting to create test data\n",
        "X_train, X_test, y_train, y_test, vocabulary, tags = analyze_and_preprocess(df, test_size=0.9)\n",
        "# load model from pickle file\n",
        "model= load_model_from_save(\"ner_model_with_params_50_300_0.01_39953_0.h5\")\n",
        "# set evaluation metrics default\n",
        "set_dafault_evaluation_parameters()\n",
        "# test model\n",
        "test_model(model,X_test,y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXGyM1KZMHmg"
      },
      "source": [
        "\n",
        "def process_test_data(data, vocabulary, tags, maxlen=200):\n",
        "  print(vocabulary)\n",
        "  word_to_ids = {w: i for i, w in enumerate(vocabulary)}\n",
        "  tag_to_ids = {t: i for i, t in enumerate(tags)}\n",
        "  ids_to_tags = {v: k for k, v in iteritems(tag_to_ids)}\n",
        "  X = [[word_to_ids[w.lower()] for w in s.split()] for s in data]\n",
        "  X = pad_sequences(maxlen=max_len_seq, sequences=X, padding=\"post\",value=number_of_words - 1)\n",
        "  return X, tag_to_ids, ids_to_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOs-dGFjnKop",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "3b9d9afd-38c0-48e6-ebb1-8a4b52326324"
      },
      "source": [
        "model= load_model_from_save(\"ner_model_with_params_100_300_0.01_975_0.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2d811238b2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mload_model_from_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_model_with_params_100_300_0.01_975_0.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_wiht_single_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ben ilker Fener, Marmara Üniversitesi'nde Bilgisayar Mühendisliği 4. sınıf öğrencisiyim.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-4eb64fc7a795>\u001b[0m in \u001b[0;36mtest_wiht_single_sentence\u001b[0;34m(model, sentence, max_len_seq)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0mrevised_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mpunc_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'['\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m']'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set('!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunc_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m       \u001b[0mrevised_sentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "Vt9YnuQKr5jD",
        "outputId": "8dd5b1ee-ea3f-4ed8-bb76-69df710b99f4"
      },
      "source": [
        "test_wiht_single_sentence(model, \"Ben ilker Fener, Marmara Üniversitesi'nde Bilgisayar Mühendisliği 4. sınıf öğrencisiyim.\", 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-60fa60db4580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_wiht_single_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ben ilker Fener, Marmara Üniversitesi'nde Bilgisayar Mühendisliği 4. sınıf öğrencisiyim.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-7cf643a5d408>\u001b[0m in \u001b[0;36mtest_wiht_single_sentence\u001b[0;34m(model, sentence, max_len_seq)\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_8 to have shape (100,) but got array with shape (1,)"
          ]
        }
      ]
    }
  ]
}