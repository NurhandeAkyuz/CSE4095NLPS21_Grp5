{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_deep_learning_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPn_1ES7zLwa",
        "outputId": "17620b78-944b-4037-d8cf-f44825885644"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip uninstall keras\n",
        "!pip install tensorflow==1.15\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (56.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=c15a53308f99f04a258782bfc9b06492da027b521f3ad2f407d1a4337ee3936b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires keras>=2.0.0, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, keras-applications, gast, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xNNPMUvCPVM",
        "outputId": "8bcb1863-c52f-44a3-a613-d7c6191c5cf9"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "import os\n",
        "print(keras.__version__)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "from math import nan\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.layers import CRF\n",
        "\n",
        "will_parsed_row_of_corpus = 40000"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-0irz10ou\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-0irz10ou\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=9091dd417350968c5c4353e44f521980e3425e35337e598d4946d17559d1ef53\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tvy9njeb/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQAUGoukCPVU"
      },
      "source": [
        "def get_path(*args):\n",
        "    spec_path = os.sep.join(args)\n",
        "    return os.path.join(os.getcwd(), spec_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBwIEj3iwcD",
        "outputId": "480960d1-4a09-442e-d097-6d3c1795d1dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGfbmNhxCPVW",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "a0c81d95-73ff-4e6b-a210-115b5f895b9d"
      },
      "source": [
        "# read file and create data frame from it\n",
        "\n",
        "file_path = get_path(\"drive\",\"MyDrive\",\"TWNERTC_TC_Coarse Grained NER_DomainIndependent_NoiseReduction.DUMP\") \n",
        "\n",
        "f = open(file_path, \"r\")\n",
        "\n",
        "line_count = 0\n",
        "\n",
        "data = dict()\n",
        "data[\"sentence_id\"] = list()\n",
        "data[\"tag\"] = list()\n",
        "data[\"word\"] = list()\n",
        "\n",
        "sentence_id_arr = data[\"sentence_id\"]\n",
        "tag_arr = data[\"tag\"]\n",
        "word_arr = data[\"word\"]\n",
        "\n",
        "\n",
        "for line in f.readlines():\n",
        "\n",
        "    if line_count > will_parsed_row_of_corpus:\n",
        "      break\n",
        "\n",
        "    line_count += 1\n",
        "    # each line seperated by ht (horizontal tabs)\n",
        "    splitted = line.split(\"\\t\")\n",
        "    \n",
        "    if len(splitted) ==3:\n",
        "      tag_split = splitted[1].split(\" \")\n",
        "      word_split = splitted[2].split(\" \")\n",
        "    \n",
        "      for tag, word in zip(tag_split, word_split):\n",
        "          word = word.strip()\n",
        "          if word[len(word)-1] == \"\\n\":\n",
        "              word = word[:-1]\n",
        "          \n",
        "          sentence_id_arr.append(line_count)\n",
        "          tag_arr.append(tag)\n",
        "          word_arr.append(word)\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"sentence_id\", \"tag\", \"word\"])\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>tag</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B-PERSON</td>\n",
              "      <td>Corina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I-PERSON</td>\n",
              "      <td>Casanova</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>O</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>B-LOCATION</td>\n",
              "      <td>İsviçre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>O</td>\n",
              "      <td>Federal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_id         tag      word\n",
              "0            1    B-PERSON    Corina\n",
              "1            1    I-PERSON  Casanova\n",
              "2            1           O         ,\n",
              "3            1  B-LOCATION   İsviçre\n",
              "4            1           O   Federal"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cATxMUWjCPVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78fdb97-9672-4154-d725-f3da1335e349"
      },
      "source": [
        "df.shape[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "673297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R_IO7U_CPVX"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        self.n_sent = 1\n",
        "        self.dataset = dataset\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
        "                                                        s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.dataset.groupby(\"sentence_id\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnW1uotZCPVY"
      },
      "source": [
        "getter = SentenceGetter(df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55-TAeauCPVY"
      },
      "source": [
        "sentences = getter.sentences"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67euaDS9CPVY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec625bab-a483-4e5b-b7fa-eb253d356bcf"
      },
      "source": [
        "print(f\"Number of sentences in data set {len(sentences)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in data set 40001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvG8TnmuCPVZ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd90dd1-bdc1-4e5d-c349-b9890cdf0aac"
      },
      "source": [
        "sentences[10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Denton', 'B-LOCATION'),\n",
              " (',', 'O'),\n",
              " ('Amerika', 'B-LOCATION'),\n",
              " ('Birleşik', 'I-LOCATION'),\n",
              " (\"Devletleri'nde\", 'I-LOCATION'),\n",
              " ('Teksas', 'B-LOCATION'),\n",
              " ('eyaletinin', 'O'),\n",
              " ('Denton', 'B-LOCATION'),\n",
              " ('bölgesindeki', 'O'),\n",
              " ('bir', 'O'),\n",
              " ('şehirdir', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxzcLIL0CPVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebdb4ae-33e6-435d-cc77-adbd808af1fa"
      },
      "source": [
        "# check length of longest sentence \n",
        "max_len = max([len(s) for s in sentences])\n",
        "print ('Maximum sequence length:', max_len)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90_fre6yCPVa"
      },
      "source": [
        "# visualize the length of sentences in data\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnOBOYDQCPVa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ac246df6-3e9e-469e-d6d8-d3417cfafc3c"
      },
      "source": [
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEhG1bK0CPVa"
      },
      "source": [
        "# create set of unique words and add ENDPAD as last element\n",
        "words = list(set(df[\"word\"].values))\n",
        "words.append(\"ENDPAD\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O87ZeJxyCPVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ad19fe-dd79-4b84-9593-ed965fa157db"
      },
      "source": [
        "n_words = len(words) \n",
        "n_words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY24srhFCPVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772aed35-7b24-4a8f-9e7f-35a7ea20556a"
      },
      "source": [
        "tags = list(set(df[\"tag\"].values))\n",
        "tags"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I-LOCATION',\n",
              " 'B-ORGANIZATION',\n",
              " 'I-MISC',\n",
              " 'B-PERSON',\n",
              " 'B-LOCATION',\n",
              " 'B-MISC',\n",
              " 'I-ORGANIZATION',\n",
              " 'O',\n",
              " 'I-PERSON']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFrUAWSPCPVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4613759-c86c-4fc4-859f-709dd7f6f91c"
      },
      "source": [
        "n_tags = len(tags); n_tags"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovb9DJjwCPVc"
      },
      "source": [
        "# creating index dictionaries for words and tags\n",
        "from future.utils import iteritems\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "idx2tag = {v: k for k, v in iteritems(tag2idx)}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p2bWgKNCPVc"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X = pad_sequences(maxlen=50, sequences=X, padding=\"post\",value=n_words - 1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZGeC-dCPVc"
      },
      "source": [
        "# We append \"O\" tags end of each sentence that smaller than 50.\n",
        "y_idx = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=50, sequences=y_idx, padding=\"post\", value=tag2idx[\"O\"])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48VUazCtCPVd",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35f0302-be2a-477c-f673-29d74af6873f"
      },
      "source": [
        "print(y_idx[2])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 8, 7, 5, 7, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr84bwPLCPVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce8bdc6-02ca-49c3-a7d7-5529430a24ce"
      },
      "source": [
        "len(y) # it should be equal longest sentence length"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlI8-FEYCPVe"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16exyg9XCPVe"
      },
      "source": [
        "# split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSu4lCPiCPVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b24743-3ed3-472a-8251-829c6f564edc"
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "import keras as k\n",
        "import time\n",
        "print(k.__version__)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swn_R9-bCPVf"
      },
      "source": [
        "# input shape should be equal to the longest input length\n",
        "def prepare_model(input_shape, embedding_size, learning_rate):\n",
        "  input = Input(shape=(input_shape,))\n",
        "  word_embedding_size = embedding_size\n",
        "  model = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=input_shape)(input)\n",
        "  model = Bidirectional(LSTM(units=word_embedding_size, \n",
        "                            return_sequences=True, \n",
        "                            dropout=0.5, \n",
        "                            recurrent_dropout=0.5, \n",
        "                            kernel_initializer=k.initializers.he_normal()))(model)\n",
        "  model = LSTM(units=word_embedding_size * 2, \n",
        "              return_sequences=True, \n",
        "              dropout=0.5, \n",
        "              recurrent_dropout=0.5, \n",
        "              kernel_initializer=k.initializers.he_normal())(model)\n",
        "  model = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  # previously softmax output layer\n",
        "\n",
        "  crf = CRF(n_tags)  # CRF layer\n",
        "  out = crf(model)  # output\n",
        "\n",
        "  model = Model(input, out)\n",
        "\n",
        "  adam = k.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "  #model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REurKNOVWjTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd35fdb-24d5-41e0-cf52-2202f7ed83e1"
      },
      "source": [
        "model = prepare_model(max_len,300, 0.01)\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 50, 300)           31159200  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 50, 600)           1442400   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50, 600)           2882400   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 50, 9)             5409      \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 50, 9)             189       \n",
            "=================================================================\n",
            "Total params: 35,489,598\n",
            "Trainable params: 35,489,598\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE2dsRYjCPVg"
      },
      "source": [
        "# Saving the best only\n",
        "def train_model(model):\n",
        "  \"\"\"filepath=get_path(\"ner-bi-lstm-td-model-{val_acc:.2f}.hdf5\")\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "  callbacks_list = [checkpoint] -> , callbacks=callbacks_list\"\"\"\n",
        "  history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=20, validation_split=0.2, verbose=1)\n",
        "  return history, model\n",
        "\n",
        "def save_model(model, *attr):\n",
        "  timestamp = int(round(time.time() * 1000))\n",
        "  file_name = \"ner_bi_lstm_crf_model_\"+\"_\".join(attr)+f\"_{timestamp}.h5\"\n",
        "  model_save_path = get_path(file_name)\n",
        "  model.save_weights(model_save_path)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3cDN3oCPVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78a611d-e632-466a-b82d-d47dfa018dd9"
      },
      "source": [
        "history, trained_model = train_model(model)\n",
        "save_model(trained_model,\"50\",\"300\",\"0.01\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 25600 samples, validate on 6400 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " 2816/25600 [==>...........................] - ETA: 17:53 - loss: 0.7498 - crf_viterbi_accuracy: 0.7630 - acc: 0.0015"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxOWxSZ4W2ET"
      },
      "source": [
        "# model evaluation metrics\n",
        "def set_default_evulation_metrics():\n",
        "  TP = {}\n",
        "  TN = {}\n",
        "  FP = {}\n",
        "  FN = {}\n",
        "  \n",
        "  for tag in tag2idx.keys():\n",
        "      TP[tag] = 0\n",
        "      TN[tag] = 0    \n",
        "      FP[tag] = 0    \n",
        "      FN[tag] = 0    \n",
        "\n",
        "\n",
        "def accumulate_score_by_tag(gt, pred):\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  For each tag keep stats\n",
        "  \"\"\"\n",
        "  if gt == pred:\n",
        "      TP[gt] += 1\n",
        "  elif gt != 'O' and pred == 'O':\n",
        "      FN[gt] +=1\n",
        "  elif gt == 'O' and pred != 'O':\n",
        "      FP[gt] += 1\n",
        "  else:\n",
        "      TN[gt] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5Bv6mrCPVh"
      },
      "source": [
        "# Single prediction and verbose results\n",
        "i = 10\n",
        "p = model.predict(np.array([X_test[i]]))\n",
        "p = np.argmax(p, axis=-1)\n",
        "gt = np.argmax(y_test[i], axis=-1)\n",
        "print(gt)\n",
        "print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
        "for idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n",
        "    #\n",
        "    print(\"{:14}: ({:5}): {}\".format(words[w],idx2tag[gt[idx]],tags[pred]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtHVSpNNXIfh"
      },
      "source": [
        "def test_model(model,x_test,y_test):\n",
        "  p = model.predict(np.array(x_test))\n",
        "\n",
        "  from sklearn.metrics import classification_report\n",
        "\n",
        "  print(classification_report(np.argmax(y_test, 2).ravel(), np.argmax(p, axis=2).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))\n",
        "  \n",
        "  set_default_evulation_metrics()\n",
        "  # acumulate the scores by tag\n",
        "  for i, sentence in enumerate(x_test):\n",
        "      y_hat = np.argmax(p[i], axis=-1)\n",
        "      gt = np.argmax(y_test[i], axis=-1)\n",
        "      for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
        "          accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])\n",
        "\n",
        "  for tag in tag2idx.keys():\n",
        "      print(f'tag:{tag}')    \n",
        "      print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
        "      print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pob-9rDRXLdg"
      },
      "source": [
        "test_model(model,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_DphIjLSe-2"
      },
      "source": [
        "# load model from save and test\n",
        "from keras.models import load_model\n",
        "model = prepare_model(50,300,0.01)\n",
        "trained_model_path = get_path(\"ner_bi_lstm_crf_model_50_300_0.01_1621337204296.h5\")\n",
        "model.load_weights(trained_model_path)\n",
        "test_model(model,X_test,y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}